# Fly.io configuration for Python AI Service
app = "trading-bot-ai-service"
primary_region = "sin"  # Singapore region for low latency

[build]
  dockerfile = "Dockerfile.production"

[env]
  PORT = "8000"
  PYTHONPATH = "/app"
  MODEL_CACHE_DIR = "/app/models/saved"
  LOG_LEVEL = "INFO"

[http_service]
  internal_port = 8000
  force_https = true
  auto_stop_machines = false
  auto_start_machines = true
  min_machines_running = 1
  processes = ["app"]

  [http_service.concurrency]
    type = "connections"
    hard_limit = 500
    soft_limit = 400

[[services]]
  protocol = "tcp"
  internal_port = 8000
  processes = ["app"]

  [[services.ports]]
    port = 80
    handlers = ["http"]
    force_https = true

  [[services.ports]]
    port = 443
    handlers = ["tls", "http"]

  [services.concurrency]
    type = "connections"
    hard_limit = 500
    soft_limit = 400

# Health checks
[checks]
  [checks.alive]
    grace_period = "15s"
    interval = "30s"
    method = "GET"
    path = "/health"
    port = 8000
    protocol = "http"
    timeout = "10s"
    type = "http"

# Resource allocation - AI service needs more memory
[vm]
  cpu_kind = "shared"
  cpus = 1
  memory_mb = 2048

# Restart policy
[restart]
  policy = "on-failure"

# Volume for model caching (optional)
[[mounts]]
  source = "ai_models"
  destination = "/app/models/saved"
  initial_size = "1GB" 