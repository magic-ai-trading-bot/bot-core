# Bot Core - Full VPS Deployment (16GB+ RAM)
# All services including messaging, monitoring, API gateway, MCP, and OpenClaw

services:
  # ============================================
  # CORE SERVICES
  # ============================================

  # MongoDB - Primary database
  mongodb:
    image: mongo:7.0
    container_name: mongodb
    restart: unless-stopped
    command: ["--bind_ip_all", "--port", "27017"]
    environment:
      - MONGO_INITDB_ROOT_USERNAME=${MONGO_ROOT_USER:-admin}
      - MONGO_INITDB_ROOT_PASSWORD=${MONGO_ROOT_PASSWORD:-secure_password_123}
      - MONGO_INITDB_DATABASE=bot_core
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db
      - mongodb_config:/data/configdb
    networks:
      - bot-network
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: "2"

  # Python AI Service
  python-ai-service:
    build:
      context: ./python-ai-service
      dockerfile: Dockerfile
    container_name: python-ai-service
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - DATABASE_URL=mongodb://${MONGO_ROOT_USER:-admin}:${MONGO_ROOT_PASSWORD:-secure_password_123}@mongodb:27017/bot_core?authSource=admin
      - RUST_API_URL=http://rust-core-engine:8080
      - RABBITMQ_USER=${RABBITMQ_USER:-admin}
      - RABBITMQ_PASSWORD=${RABBITMQ_PASSWORD:-rabbitmq_default_password}
      - RABBITMQ_HOST=rabbitmq
      - RABBITMQ_PORT=5672
      - RABBITMQ_VHOST=bot-core
      - REDIS_HOST=redis-cache
      - REDIS_PASSWORD=${REDIS_PASSWORD:-redis_default_password}
    volumes:
      - ./python-ai-service/models:/app/models
      - ./python-ai-service/logs:/app/logs
      - ./python-ai-service/config.yaml:/app/config.yaml
    networks:
      - bot-network
    depends_on:
      mongodb:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: "2"

  # Rust Core Engine
  rust-core-engine:
    build:
      context: ./rust-core-engine
      dockerfile: Dockerfile
    container_name: rust-core-engine
    restart: unless-stopped
    ports:
      - "8080:8080"
    environment:
      - RUST_LOG=info
      - DATABASE_URL=mongodb://${MONGO_ROOT_USER:-admin}:${MONGO_ROOT_PASSWORD:-secure_password_123}@mongodb:27017/bot_core?authSource=admin
      - PYTHON_AI_SERVICE_URL=http://python-ai-service:8000
      - BINANCE_API_KEY=${BINANCE_API_KEY:-}
      - BINANCE_SECRET_KEY=${BINANCE_SECRET_KEY:-}
      - BINANCE_TESTNET=false
      - TRADING_ENABLED=false
      - JWT_SECRET=${JWT_SECRET:-default_jwt_secret_change_in_production}
    volumes:
      - ./rust-core-engine/data:/app/data
      - ./rust-core-engine/logs:/app/logs
      - ./rust-core-engine/config.toml:/app/config.toml
    networks:
      - bot-network
    depends_on:
      python-ai-service:
        condition: service_healthy
      mongodb:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: "2"

  # Next.js UI Dashboard
  nextjs-ui-dashboard:
    build:
      context: ./nextjs-ui-dashboard
      dockerfile: Dockerfile
    container_name: nextjs-ui-dashboard
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - VITE_RUST_API_URL=http://180.93.2.247:8080
      - VITE_PYTHON_AI_URL=http://180.93.2.247:8000
      - VITE_WS_URL=ws://180.93.2.247:8080/ws
    networks:
      - bot-network
    depends_on:
      rust-core-engine:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "0.5"

  # ============================================
  # MESSAGING & ASYNC TASKS
  # ============================================

  # Redis - Cache and Celery result backend
  redis:
    image: redis:7-alpine
    container_name: redis-cache
    restart: unless-stopped
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD:-redis_default_password}
    volumes:
      - redis_data:/data
    networks:
      - bot-network
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD:-redis_default_password}", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.5"

  # RabbitMQ - Message queue for async processing
  rabbitmq:
    image: rabbitmq:3.12-management-alpine
    container_name: rabbitmq
    restart: unless-stopped
    environment:
      - RABBITMQ_DEFAULT_USER=${RABBITMQ_USER:-admin}
      - RABBITMQ_DEFAULT_PASS=${RABBITMQ_PASSWORD:-rabbitmq_default_password}
      - RABBITMQ_DEFAULT_VHOST=bot-core
    ports:
      - "5672:5672"
      - "15672:15672"
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    networks:
      - bot-network
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "1"

  # Celery Worker - Process async tasks
  celery-worker:
    build:
      context: ./python-ai-service
      dockerfile: Dockerfile
    container_name: celery-worker
    restart: unless-stopped
    command: celery -A celery_app worker --loglevel=info --concurrency=4 --max-tasks-per-child=50
    environment:
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - RABBITMQ_USER=${RABBITMQ_USER:-admin}
      - RABBITMQ_PASSWORD=${RABBITMQ_PASSWORD:-rabbitmq_default_password}
      - RABBITMQ_HOST=rabbitmq
      - RABBITMQ_PORT=5672
      - RABBITMQ_VHOST=bot-core
      - REDIS_HOST=redis-cache
      - REDIS_PASSWORD=${REDIS_PASSWORD:-redis_default_password}
      - DATABASE_URL=mongodb://${MONGO_ROOT_USER:-admin}:${MONGO_ROOT_PASSWORD:-secure_password_123}@mongodb:27017/bot_core?authSource=admin
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - RUST_API_URL=http://rust-core-engine:8080
      - PYTHON_API_URL=http://python-ai-service:8000
    volumes:
      - ./python-ai-service/logs:/app/logs
      - ./python-ai-service/models:/app/models
    networks:
      - bot-network
    depends_on:
      rabbitmq:
        condition: service_healthy
      redis:
        condition: service_healthy
      mongodb:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "celery -A celery_app inspect ping -t 10 || exit 1"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 90s
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: "2"

  # Celery Beat - Scheduler for periodic tasks
  celery-beat:
    build:
      context: ./python-ai-service
      dockerfile: Dockerfile
    container_name: celery-beat
    restart: unless-stopped
    command: celery -A celery_app beat --loglevel=info --schedule=/tmp/celerybeat-schedule --pidfile=/tmp/celerybeat.pid
    environment:
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - RABBITMQ_USER=${RABBITMQ_USER:-admin}
      - RABBITMQ_PASSWORD=${RABBITMQ_PASSWORD:-rabbitmq_default_password}
      - RABBITMQ_HOST=rabbitmq
      - RABBITMQ_PORT=5672
      - RABBITMQ_VHOST=bot-core
      - REDIS_HOST=redis-cache
      - REDIS_PASSWORD=${REDIS_PASSWORD:-redis_default_password}
      - DATABASE_URL=mongodb://${MONGO_ROOT_USER:-admin}:${MONGO_ROOT_PASSWORD:-secure_password_123}@mongodb:27017/bot_core?authSource=admin
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - RUST_API_URL=http://rust-core-engine:8080
      - PYTHON_API_URL=http://python-ai-service:8000
    volumes:
      - ./python-ai-service/logs:/app/logs
    networks:
      - bot-network
    depends_on:
      rabbitmq:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "test -f /tmp/celerybeat.pid && kill -0 $(cat /tmp/celerybeat.pid) 2>/dev/null || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "0.5"

  # Flower - Celery monitoring dashboard
  flower:
    build:
      context: ./python-ai-service
      dockerfile: Dockerfile
    container_name: flower
    restart: unless-stopped
    command: celery -A celery_app flower --port=5555 --basic_auth=${FLOWER_USER:-admin}:${FLOWER_PASSWORD:-admin}
    environment:
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
      - MPLCONFIGDIR=/tmp/matplotlib
      - RABBITMQ_USER=${RABBITMQ_USER:-admin}
      - RABBITMQ_PASSWORD=${RABBITMQ_PASSWORD:-rabbitmq_default_password}
      - RABBITMQ_HOST=rabbitmq
      - RABBITMQ_VHOST=bot-core
      - REDIS_HOST=redis-cache
      - REDIS_PASSWORD=${REDIS_PASSWORD:-redis_default_password}
      - DATABASE_URL=mongodb://${MONGO_ROOT_USER:-admin}:${MONGO_ROOT_PASSWORD:-secure_password_123}@mongodb:27017/bot_core?authSource=admin
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - RUST_API_URL=http://rust-core-engine:8080
    volumes:
      - ./python-ai-service/logs:/app/logs
      - flower_tmp:/tmp
    ports:
      - "5555:5555"
    networks:
      - bot-network
    depends_on:
      rabbitmq:
        condition: service_healthy
      celery-worker:
        condition: service_started
    healthcheck:
      test: ["CMD-SHELL", "curl -f -u ${FLOWER_USER:-admin}:${FLOWER_PASSWORD:-admin} http://localhost:5555/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 256M

  # ============================================
  # API GATEWAY
  # ============================================

  # Kong Database (PostgreSQL)
  kong-database:
    image: postgres:13-alpine
    container_name: kong-database
    restart: unless-stopped
    environment:
      - POSTGRES_DB=kong
      - POSTGRES_USER=kong
      - POSTGRES_PASSWORD=${KONG_DB_PASSWORD:-kong_db_default_password}
    volumes:
      - kong_data:/var/lib/postgresql/data
    networks:
      - bot-network
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "kong"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.5"

  # Kong Migration (runs once)
  kong-migration:
    image: kong:3.8
    container_name: kong-migration
    command: kong migrations bootstrap
    depends_on:
      kong-database:
        condition: service_healthy
    environment:
      - KONG_DATABASE=postgres
      - KONG_PG_HOST=kong-database
      - KONG_PG_USER=kong
      - KONG_PG_PASSWORD=${KONG_DB_PASSWORD:-kong_db_default_password}
    networks:
      - bot-network
    restart: on-failure

  # Kong API Gateway
  kong:
    image: kong:3.8
    container_name: kong
    restart: unless-stopped
    depends_on:
      kong-database:
        condition: service_healthy
      kong-migration:
        condition: service_completed_successfully
    environment:
      - KONG_DATABASE=postgres
      - KONG_PG_HOST=kong-database
      - KONG_PG_USER=kong
      - KONG_PG_PASSWORD=${KONG_DB_PASSWORD:-kong_db_default_password}
      - KONG_PROXY_ACCESS_LOG=/dev/stdout
      - KONG_ADMIN_ACCESS_LOG=/dev/stdout
      - KONG_PROXY_ERROR_LOG=/dev/stderr
      - KONG_ADMIN_ERROR_LOG=/dev/stderr
      - KONG_ADMIN_LISTEN=0.0.0.0:8001
    ports:
      - "8100:8000"
      - "8443:8443"
      - "8001:8001"
      - "8444:8444"
    networks:
      - bot-network
    healthcheck:
      test: ["CMD", "kong", "health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "1"

  # ============================================
  # MONITORING
  # ============================================

  # Prometheus - Metrics collection
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./infrastructure/monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    networks:
      - bot-network
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "1"

  # Grafana - Monitoring dashboard
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./infrastructure/monitoring/grafana:/etc/grafana/provisioning:ro
    networks:
      - bot-network
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.5"

  # ============================================
  # MCP & OPENCLAW (Phase 01/04)
  # ============================================

  # OpenClaw Gateway - AI assistant via Telegram/WhatsApp
  openclaw:
    build:
      context: ./openclaw
      dockerfile: Dockerfile
    container_name: openclaw
    restart: unless-stopped
    environment:
      - HOME=/home/node
      - TERM=xterm-256color
      - OPENCLAW_GATEWAY_TOKEN=${OPENCLAW_GATEWAY_TOKEN:-}
      - CLAUDE_AI_SESSION_KEY=${CLAUDE_AI_SESSION_KEY:-}
      - CLAUDE_WEB_SESSION_KEY=${CLAUDE_WEB_SESSION_KEY:-}
      - CLAUDE_WEB_COOKIE=${CLAUDE_WEB_COOKIE:-}
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN:-}
      - TELEGRAM_USER_ID=${TELEGRAM_USER_ID:-}
      - WHATSAPP_PHONE_NUMBER=${WHATSAPP_PHONE_NUMBER:-}
      - MCP_URL=http://mcp-server:8090
      - MCP_AUTH_TOKEN=${MCP_AUTH_TOKEN:-}
      - NODE_ENV=production
    volumes:
      - ./openclaw/config/openclaw.json:/home/node/.openclaw/openclaw.json:ro
      - ./openclaw/config/cron:/home/node/.openclaw/cron:ro
      - ./openclaw/workspace:/home/node/.openclaw/workspace:ro
      - openclaw_data:/data/openclaw
    networks:
      - bot-network
    depends_on:
      mcp-server:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:18789/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 768M
          cpus: "0.5"

  # MCP Server - Bridges OpenClaw/Claude to BotCore APIs
  mcp-server:
    build:
      context: ./mcp-server
      dockerfile: Dockerfile
    container_name: mcp-server
    restart: unless-stopped
    environment:
      - MCP_AUTH_TOKEN=${MCP_AUTH_TOKEN:-}
      - RUST_API_URL=http://rust-core-engine:8080
      - PYTHON_API_URL=http://python-ai-service:8000
      - PROMETHEUS_URL=http://prometheus:9090
      - BOTCORE_EMAIL=${BOTCORE_EMAIL:-}
      - BOTCORE_PASSWORD=${BOTCORE_PASSWORD:-}
      - MCP_PORT=8090
      - NODE_ENV=production
    networks:
      - bot-network
    depends_on:
      rust-core-engine:
        condition: service_healthy
      python-ai-service:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8090/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "0.5"

networks:
  bot-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  mongodb_data:
  mongodb_config:
  redis_data:
  rabbitmq_data:
  kong_data:
  prometheus_data:
  grafana_data:
  flower_tmp:
  openclaw_data:
