name: Test Coverage & Quality Gates

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

permissions:
  contents: read
  issues: write
  pull-requests: write
  checks: write

env:
  RUST_COVERAGE_THRESHOLD: 90.0
  PYTHON_COVERAGE_THRESHOLD: 95.0
  FRONTEND_COVERAGE_THRESHOLD: 95.0

jobs:
  test-rust-lib:
    name: Rust Library Tests + Coverage
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          override: true
          components: rustfmt, clippy

      - name: Free up disk space
        uses: jlumbroso/free-disk-space@main
        with:
          tool-cache: false
          android: true
          dotnet: true
          haskell: true
          large-packages: false
          docker-images: true
          swap-storage: false

      - name: Cache cargo registry
        uses: actions/cache@v5
        continue-on-error: true
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
          key: ${{ runner.os }}-cargo-v3-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-v3-

      - name: Cache cargo target
        uses: actions/cache@v5
        continue-on-error: true
        with:
          path: rust-core-engine/target
          key: ${{ runner.os }}-cargo-target-lib-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-target-lib-

      - name: Install llvm-tools and cargo-llvm-cov
        run: |
          rustup component add llvm-tools-preview
          cargo install cargo-llvm-cov

      - name: Run tests with coverage (single run)
        working-directory: ./rust-core-engine
        run: |
          # Run tests ONCE with coverage instrumentation
          mkdir -p coverage
          cargo llvm-cov --lib --cobertura --output-path coverage/cobertura.xml

      - name: Generate additional coverage reports (no re-run)
        working-directory: ./rust-core-engine
        run: |
          # Generate reports from existing profdata (does NOT re-run tests)
          cargo llvm-cov report --lcov --output-path coverage/lcov.info
          cargo llvm-cov report --html --output-dir coverage

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5
        with:
          files: ./rust-core-engine/coverage/cobertura.xml
          flags: rust
          name: rust-coverage
          fail_ci_if_error: false

      - name: Check coverage threshold
        working-directory: ./rust-core-engine
        run: |
          # Uses existing profdata (no re-run)
          COVERAGE=$(cargo llvm-cov report 2>/dev/null | grep "TOTAL" | awk '{for(i=1;i<=NF;i++) if($i ~ /%/) last=$i; print last}' | sed 's/%//' || echo "0")
          echo "Current Rust coverage: $COVERAGE%"
          echo "Required threshold: $RUST_COVERAGE_THRESHOLD%"

          if (( $(echo "$COVERAGE < $RUST_COVERAGE_THRESHOLD" | bc -l) )); then
            echo "❌ Coverage $COVERAGE% is below $RUST_COVERAGE_THRESHOLD% threshold"
            exit 1
          else
            echo "✅ Coverage $COVERAGE% meets threshold"
          fi

      - name: Upload coverage HTML report
        uses: actions/upload-artifact@v6
        with:
          name: rust-coverage-report
          path: rust-core-engine/coverage/

  test-rust-integration:
    name: Rust Integration ${{ matrix.name }}
    runs-on: ubuntu-latest
    timeout-minutes: 30
    strategy:
      fail-fast: false
      matrix:
        include:
          - shard: 1
            name: "Trading Core"
            tests: "--test test_paper_trading --test test_paper_trading_coverage --test test_paper_trading_engine --test test_trading --test test_trading_engine_coverage"
          - shard: 2
            name: "Strategies & Risk"
            tests: "--test test_strategies --test test_all_5_strategies_live --test test_risk_management --test test_position_risk_comprehensive --test test_trailing_stops --test test_indicators_comprehensive"
          - shard: 3
            name: "API & Auth"
            tests: "--test test_api_handlers --test test_auth --test test_auth_coverage --test test_security_handlers --test test_notifications_settings"
          - shard: 4
            name: "Data & Storage"
            tests: "--test test_storage --test test_storage_coverage --test test_fn_storage_simple --test test_market_data --test test_market_data_processor --test test_config"
          - shard: 5
            name: "Services & WebSocket"
            tests: "--test test_cross_service --test test_service_integration --test test_binance_client --test test_binance_client_coverage --test test_websocket"
          - shard: 6
            name: "AI & Real Trading"
            tests: "--test test_coverage_boost_3 --test test_real_trading_integration --test test_ai"

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          override: true

      - name: Free up disk space
        uses: jlumbroso/free-disk-space@main
        with:
          tool-cache: false
          android: true
          dotnet: true
          haskell: true
          large-packages: false
          docker-images: true
          swap-storage: false

      - name: Cache cargo registry
        uses: actions/cache@v5
        continue-on-error: true
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
          key: ${{ runner.os }}-cargo-v3-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-v3-

      - name: Cache cargo target
        uses: actions/cache@v5
        continue-on-error: true
        with:
          path: rust-core-engine/target
          key: ${{ runner.os }}-cargo-target-shard${{ matrix.shard }}-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-target-shard${{ matrix.shard }}-

      - name: Run integration tests (Shard ${{ matrix.shard }})
        working-directory: ./rust-core-engine
        run: |
          cargo test ${{ matrix.tests }} --verbose

  test-python:
    name: Python AI Service Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.11'

      - name: Cache pip packages
        uses: actions/cache@v5
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        working-directory: ./python-ai-service
        run: |
          python -m pip install --upgrade pip
          # Use lightweight CI requirements (no TensorFlow/PyTorch)
          pip install -r requirements-ci.txt
          pip install pytest pytest-cov pytest-asyncio pytest-xdist

      - name: Run Python tests (excluding ML and test_main*.py)
        working-directory: ./python-ai-service
        run: |
          # Run tests and capture results
          set +e  # Don't exit immediately on error
          python -m pytest tests/ \
            -v \
            --ignore=tests/test_ml_compatibility.py \
            --ignore=tests/test_ml_performance.py \
            --ignore=tests/test_main.py \
            --ignore=tests/test_main_comprehensive.py \
            --cov=. \
            --cov-config=.coveragerc \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term-missing
          TEST_EXIT_CODE=$?
          set -e

          # Fail if tests failed
          if [ $TEST_EXIT_CODE -ne 0 ]; then
            echo "❌ Tests failed with exit code $TEST_EXIT_CODE"
            exit $TEST_EXIT_CODE
          fi

      - name: Run test_main.py and test_main_comprehensive.py separately (MongoDB tests have state pollution issues)
        working-directory: ./python-ai-service
        run: |
          # Run tests and capture results
          set +e  # Don't exit immediately on error
          python -m pytest tests/test_main.py tests/test_main_comprehensive.py \
            -v \
            --cov=. \
            --cov-config=.coveragerc \
            --cov-append \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term-missing
          TEST_EXIT_CODE=$?
          set -e

          # Fail if tests failed
          if [ $TEST_EXIT_CODE -ne 0 ]; then
            echo "❌ Tests failed with exit code $TEST_EXIT_CODE"
            exit $TEST_EXIT_CODE
          fi

      - name: Run ML tests separately
        working-directory: ./python-ai-service
        run: |
          # Run tests and capture results
          set +e  # Don't exit immediately on error
          python -m pytest tests/test_ml_compatibility.py tests/test_ml_performance.py \
            -v \
            --cov=. \
            --cov-config=.coveragerc \
            --cov-append \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term-missing
          TEST_EXIT_CODE=$?
          set -e

          # Fail if tests failed
          if [ $TEST_EXIT_CODE -ne 0 ]; then
            echo "❌ Tests failed with exit code $TEST_EXIT_CODE"
            exit $TEST_EXIT_CODE
          fi

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5
        with:
          files: ./python-ai-service/coverage.xml
          flags: python
          name: python-coverage
          fail_ci_if_error: false

      - name: Check coverage threshold
        working-directory: ./python-ai-service
        run: |
          # Extract coverage from the final coverage report (already includes all test runs via --cov-append)
          if [ -f coverage.xml ]; then
            COVERAGE=$(python -c "import xml.etree.ElementTree as ET; tree = ET.parse('coverage.xml'); root = tree.getroot(); print(f\"{float(root.attrib['line-rate']) * 100:.2f}\")")
          else
            echo "❌ coverage.xml not found"
            exit 1
          fi

          echo "Current Python coverage: $COVERAGE%"
          echo "Required threshold: $PYTHON_COVERAGE_THRESHOLD%"

          if (( $(echo "$COVERAGE < $PYTHON_COVERAGE_THRESHOLD" | bc -l) )); then
            echo "❌ Coverage $COVERAGE% is below $PYTHON_COVERAGE_THRESHOLD% threshold"
            exit 1
          else
            echo "✅ Coverage $COVERAGE% meets threshold"
          fi

      - name: Upload coverage HTML report
        uses: actions/upload-artifact@v6
        with:
          name: python-coverage-report
          path: python-ai-service/htmlcov/

  test-frontend:
    name: Frontend (Next.js) Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Bun
        uses: oven-sh/setup-bun@v2
        with:
          bun-version: latest

      - name: Install dependencies
        working-directory: ./nextjs-ui-dashboard
        run: bun install

      - name: Run linter
        working-directory: ./nextjs-ui-dashboard
        run: bun run lint || true

      - name: Run tests with coverage
        working-directory: ./nextjs-ui-dashboard
        run: npm run test:coverage
        env:
          CI: true

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5
        with:
          files: ./nextjs-ui-dashboard/coverage/coverage-final.json
          flags: frontend
          name: frontend-coverage
          fail_ci_if_error: false

      - name: Check coverage threshold
        working-directory: ./nextjs-ui-dashboard
        run: |
          # Extract coverage from coverage-final.json (already generated above)
          if [ -f coverage/coverage-final.json ]; then
            COVERAGE=$(node -e "
              const data = require('./coverage/coverage-final.json');
              let totalStatements = 0, coveredStatements = 0;
              for (const file of Object.values(data)) {
                const s = file.s;
                for (const key of Object.keys(s)) {
                  totalStatements++;
                  if (s[key] > 0) coveredStatements++;
                }
              }
              console.log(totalStatements > 0 ? (coveredStatements / totalStatements * 100).toFixed(2) : '0');
            ")
          else
            echo "❌ coverage-final.json not found"
            exit 1
          fi

          echo "Current Frontend coverage: $COVERAGE%"
          echo "Required threshold: $FRONTEND_COVERAGE_THRESHOLD%"

          if (( $(echo "$COVERAGE < $FRONTEND_COVERAGE_THRESHOLD" | bc -l) )); then
            echo "❌ Coverage $COVERAGE% is below $FRONTEND_COVERAGE_THRESHOLD% threshold"
            exit 1
          else
            echo "✅ Coverage $COVERAGE% meets threshold"
          fi

      - name: Upload coverage HTML report
        uses: actions/upload-artifact@v6
        with:
          name: frontend-coverage-report
          path: nextjs-ui-dashboard/coverage/

  lint-and-format:
    name: Code Quality Checks
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          components: rustfmt, clippy

      - name: Rust format check
        working-directory: ./rust-core-engine
        run: cargo fmt -- --check

      - name: Rust clippy
        working-directory: ./rust-core-engine
        run: cargo clippy -- -D warnings -A clippy::too_many_arguments

      - name: Setup Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.11'

      - name: Install Python linters
        run: |
          pip install flake8 black isort mypy

      - name: Python format check
        working-directory: ./python-ai-service
        run: |
          black --check . || true
          isort --check-only . || true

      - name: Python lint
        working-directory: ./python-ai-service
        run: flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics || true

      - name: Setup Bun
        uses: oven-sh/setup-bun@v2
        with:
          bun-version: latest

      - name: Install frontend linters
        working-directory: ./nextjs-ui-dashboard
        run: bun install

      - name: Frontend lint
        working-directory: ./nextjs-ui-dashboard
        run: bun run lint || true

  coverage-report:
    name: Generate Coverage Report
    needs: [test-rust-lib, test-rust-integration, test-python, test-frontend]
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Download Rust coverage
        uses: actions/download-artifact@v7
        with:
          name: rust-coverage-report
          path: ./coverage/rust
        continue-on-error: true

      - name: Download Python coverage
        uses: actions/download-artifact@v7
        with:
          name: python-coverage-report
          path: ./coverage/python
        continue-on-error: true

      - name: Download Frontend coverage
        uses: actions/download-artifact@v7
        with:
          name: frontend-coverage-report
          path: ./coverage/frontend
        continue-on-error: true

      - name: Generate summary report
        run: |
          echo "# Test Coverage Summary" > coverage-summary.md
          echo "" >> coverage-summary.md
          echo "## Coverage by Service" >> coverage-summary.md
          echo "" >> coverage-summary.md
          echo "| Service | Coverage | Status | Tests |" >> coverage-summary.md
          echo "|---------|----------|--------|-------|" >> coverage-summary.md
          echo "| Rust Core | - | ⏳ | - |" >> coverage-summary.md
          echo "| Python AI | - | ⏳ | - |" >> coverage-summary.md
          echo "| Frontend | - | ⏳ | - |" >> coverage-summary.md
          echo "" >> coverage-summary.md
          echo "View detailed reports in workflow artifacts." >> coverage-summary.md

      - name: Upload summary
        uses: actions/upload-artifact@v6
        with:
          name: coverage-summary
          path: coverage-summary.md

      - name: Comment PR with coverage
        uses: actions/github-script@v8
        if: github.event_name == 'pull_request'
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('coverage-summary.md', 'utf8');
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });

  security-scan:
    name: Security Vulnerability Scan
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@0.34.1
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'

      # Note: SARIF upload is disabled because it requires GitHub Advanced Security
      # for private repos (paid feature). The scan results are still available in
      # the trivy-results.sarif artifact. To enable:
      # 1. Make repo public, OR
      # 2. Enable GitHub Advanced Security in repo settings
      # 3. Uncomment the step below
      # - name: Upload Trivy results to GitHub Security
      #   uses: github/codeql-action/upload-sarif@v3
      #   with:
      #     sarif_file: 'trivy-results.sarif'

      - name: Upload Trivy scan results as artifact
        uses: actions/upload-artifact@v6
        with:
          name: trivy-scan-results-test-coverage
          path: trivy-results.sarif
          retention-days: 30

  performance-benchmarks:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable

      - name: Install criterion
        run: cargo install cargo-criterion || true

      - name: Run Rust benchmarks
        working-directory: ./rust-core-engine
        run: |
          cargo bench --no-fail-fast || echo "Benchmarks not configured yet"

      - name: Upload benchmark results
        uses: actions/upload-artifact@v6
        with:
          name: rust-benchmarks
          path: rust-core-engine/target/criterion/
        continue-on-error: true
